{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"this is a Linear regression model for Medical cost dataset. The dataset consists of age, sex, BMI(body mass index), children, smoker and region feature, which are independent and charge as a dependent feature. We will predict individual medical costs billed by health insurance.","metadata":{}},{"cell_type":"markdown","source":"# Definition\n\nLinear regression is a **supervised learining** algorithm used when target / dependent variable  **continues** real number. It establishes relationship between dependent variable $y$ and one or more independent variable $x$ using best fit line.   It work on the principle of ordinary least square $(OLS)$ / Mean square errror $(MSE)$. In statistics ols is method to estimated unkown parameter of linear regression function, it's goal is to minimize sum of square difference between observed dependent variable in the given data set and those predicted by linear regression fuction. \n\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"## Import Library and Dataset\nNow we will import couple of python library required for our analysis and import dataset ","metadata":{}},{"cell_type":"code","source":"# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\nplt.rcParams['figure.figsize'] = [8,5]\nplt.rcParams['font.size'] =14\nplt.rcParams['font.weight']= 'bold'\nplt.style.use('seaborn-whitegrid')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:44.586028Z","iopub.execute_input":"2022-05-24T13:54:44.586370Z","iopub.status.idle":"2022-05-24T13:54:45.524974Z","shell.execute_reply.started":"2022-05-24T13:54:44.586316Z","shell.execute_reply":"2022-05-24T13:54:45.524157Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Import dataset\n#path ='dataset/'\npath = '../input/'\ndf = pd.read_csv(path+'insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:45.526416Z","iopub.execute_input":"2022-05-24T13:54:45.526681Z","iopub.status.idle":"2022-05-24T13:54:45.605026Z","shell.execute_reply.started":"2022-05-24T13:54:45.526636Z","shell.execute_reply":"2022-05-24T13:54:45.604074Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=2,height=6)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:45.606454Z","iopub.execute_input":"2022-05-24T13:54:45.606836Z","iopub.status.idle":"2022-05-24T13:54:46.605381Z","shell.execute_reply.started":"2022-05-24T13:54:45.606769Z","shell.execute_reply":"2022-05-24T13:54:46.604272Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"In above plot we fit regression line into the variables.","metadata":{}},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:46.606980Z","iopub.execute_input":"2022-05-24T13:54:46.607318Z","iopub.status.idle":"2022-05-24T13:54:46.652499Z","shell.execute_reply.started":"2022-05-24T13:54:46.607257Z","shell.execute_reply":"2022-05-24T13:54:46.651664Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"###  Check for missing value","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,4))\nsns.heatmap(df.isnull(),cbar=False,cmap='viridis',yticklabels=False)\nplt.title('Missing value in the dataset');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:46.654072Z","iopub.execute_input":"2022-05-24T13:54:46.654443Z","iopub.status.idle":"2022-05-24T13:54:46.905986Z","shell.execute_reply.started":"2022-05-24T13:54:46.654371Z","shell.execute_reply":"2022-05-24T13:54:46.904841Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"There is no missing value in the data sex","metadata":{}},{"cell_type":"markdown","source":"### Plots","metadata":{}},{"cell_type":"code","source":"# correlation plot\ncorr = df.corr()\nsns.heatmap(corr, cmap = 'Wistia', annot= True);","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:46.907684Z","iopub.execute_input":"2022-05-24T13:54:46.908276Z","iopub.status.idle":"2022-05-24T13:54:47.214964Z","shell.execute_reply.started":"2022-05-24T13:54:46.908209Z","shell.execute_reply":"2022-05-24T13:54:47.213106Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Thier no correlation among valiables.","metadata":{}},{"cell_type":"code","source":"f= plt.figure(figsize=(12,4))\n\nax=f.add_subplot(121)\nsns.distplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.distplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:47.218024Z","iopub.execute_input":"2022-05-24T13:54:47.218854Z","iopub.status.idle":"2022-05-24T13:54:47.937834Z","shell.execute_reply.started":"2022-05-24T13:54:47.218438Z","shell.execute_reply":"2022-05-24T13:54:47.936731Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. for further analysis we will apply log on target variable charges. ","metadata":{}},{"cell_type":"code","source":"f = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:47.939411Z","iopub.execute_input":"2022-05-24T13:54:47.939934Z","iopub.status.idle":"2022-05-24T13:54:48.309813Z","shell.execute_reply.started":"2022-05-24T13:54:47.939704Z","shell.execute_reply":"2022-05-24T13:54:48.306799Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"From left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:48.312042Z","iopub.execute_input":"2022-05-24T13:54:48.312447Z","iopub.status.idle":"2022-05-24T13:54:48.679250Z","shell.execute_reply.started":"2022-05-24T13:54:48.312368Z","shell.execute_reply":"2022-05-24T13:54:48.678349Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"df.groupby('children').agg(['mean','min','max'])['charges']","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:48.680606Z","iopub.execute_input":"2022-05-24T13:54:48.681147Z","iopub.status.idle":"2022-05-24T13:54:48.718568Z","shell.execute_reply.started":"2022-05-24T13:54:48.681087Z","shell.execute_reply":"2022-05-24T13:54:48.717838Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(14,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:48.722522Z","iopub.execute_input":"2022-05-24T13:54:48.722866Z","iopub.status.idle":"2022-05-24T13:54:49.166444Z","shell.execute_reply.started":"2022-05-24T13:54:48.722803Z","shell.execute_reply":"2022-05-24T13:54:49.164909Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"f = plt.figure(figsize=(14,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:49.168212Z","iopub.execute_input":"2022-05-24T13:54:49.168576Z","iopub.status.idle":"2022-05-24T13:54:50.553026Z","shell.execute_reply.started":"2022-05-24T13:54:49.168505Z","shell.execute_reply":"2022-05-24T13:54:50.552119Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":">From left plot the minimum age person is insured is 18 year. There is slabs in policy most of non smoker take $1^{st}$ and $2^{nd}$ slab, for smoker policy start at $2^{nd}$ and $3^{rd}$ slab.\n\n>Body mass index (BMI) is a measure of body fat based on height and weight that applies to adult men and women. The minimum bmi is 16$kg/m^2$ and maximum upto 54$kg/m^2$","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing\n","metadata":{}},{"cell_type":"code","source":"# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.554853Z","iopub.execute_input":"2022-05-24T13:54:50.555216Z","iopub.status.idle":"2022-05-24T13:54:50.570745Z","shell.execute_reply.started":"2022-05-24T13:54:50.555150Z","shell.execute_reply":"2022-05-24T13:54:50.569766Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.572028Z","iopub.execute_input":"2022-05-24T13:54:50.572387Z","iopub.status.idle":"2022-05-24T13:54:50.585200Z","shell.execute_reply.started":"2022-05-24T13:54:50.572253Z","shell.execute_reply":"2022-05-24T13:54:50.584386Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Box -Cox transformation\n","metadata":{}},{"cell_type":"code","source":"from scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.586802Z","iopub.execute_input":"2022-05-24T13:54:50.587041Z","iopub.status.idle":"2022-05-24T13:54:50.614663Z","shell.execute_reply.started":"2022-05-24T13:54:50.587004Z","shell.execute_reply":"2022-05-24T13:54:50.613868Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.616004Z","iopub.execute_input":"2022-05-24T13:54:50.616513Z","iopub.status.idle":"2022-05-24T13:54:50.621453Z","shell.execute_reply.started":"2022-05-24T13:54:50.616465Z","shell.execute_reply":"2022-05-24T13:54:50.620517Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"The original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function.","metadata":{}},{"cell_type":"markdown","source":"## Train Test split","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.622817Z","iopub.execute_input":"2022-05-24T13:54:50.623086Z","iopub.status.idle":"2022-05-24T13:54:50.975888Z","shell.execute_reply.started":"2022-05-24T13:54:50.623028Z","shell.execute_reply":"2022-05-24T13:54:50.974808Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## Model building\nIn this step build model using our linear regression equation $\\mathbf{\\theta = (X^T X)^{-1} X^Ty}$. In first step we need to add a feature $\\mathbf{x_0 =1}$ to our original data set. ","metadata":{}},{"cell_type":"code","source":"# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) ","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.977234Z","iopub.execute_input":"2022-05-24T13:54:50.977458Z","iopub.status.idle":"2022-05-24T13:54:50.986753Z","shell.execute_reply.started":"2022-05-24T13:54:50.977424Z","shell.execute_reply":"2022-05-24T13:54:50.985457Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:50.988983Z","iopub.execute_input":"2022-05-24T13:54:50.989861Z","iopub.status.idle":"2022-05-24T13:54:51.002184Z","shell.execute_reply.started":"2022-05-24T13:54:50.989782Z","shell.execute_reply":"2022-05-24T13:54:51.000645Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\nparameter_df","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:51.004866Z","iopub.execute_input":"2022-05-24T13:54:51.005915Z","iopub.status.idle":"2022-05-24T13:54:51.228523Z","shell.execute_reply.started":"2022-05-24T13:54:51.005826Z","shell.execute_reply":"2022-05-24T13:54:51.227819Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The parameter obtained from both the model are same.So we succefull build our model using normal equation and verified using sklearn linear regression module. Let's move ahead, next step is prediction and model evaluvation.","metadata":{}},{"cell_type":"markdown","source":"## Model evaluation\n","metadata":{}},{"cell_type":"code","source":"# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:51.229667Z","iopub.execute_input":"2022-05-24T13:54:51.230091Z","iopub.status.idle":"2022-05-24T13:54:51.238750Z","shell.execute_reply.started":"2022-05-24T13:54:51.229990Z","shell.execute_reply":"2022-05-24T13:54:51.237885Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:51.240113Z","iopub.execute_input":"2022-05-24T13:54:51.240388Z","iopub.status.idle":"2022-05-24T13:54:51.254018Z","shell.execute_reply.started":"2022-05-24T13:54:51.240335Z","shell.execute_reply":"2022-05-24T13:54:51.253087Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"The model returns $R^2$ value of 77.95%, so it fit our data test very well, but still we can imporve the the performance of by diffirent technique. Please make a note that we have transformer out variable by applying  natural log. When we put model into production antilog is applied to the equation.","metadata":{}},{"cell_type":"markdown","source":"## Model Validation\nIn order to validated model we need to check few assumption of linear regression model. The common assumption for *Linear Regression* model are following\n1. Linear Relationship: In linear regression the relationship between the dependent and independent variable to be *linear*. This can be checked by scatter ploting Actual value Vs Predicted value\n2. The residual error plot should be *normally* distributed.\n3. The *mean* of *residual error* should be 0 or close to 0 as much as possible\n4. The linear regression require all variables to be multivariate normal. This assumption can best checked with Q-Q plot.\n5. Linear regession assumes that there is little or no *Multicollinearity in the data. Multicollinearity occurs when the independent variables are too highly correlated with each other. The variance inflation factor *VIF* identifies correlation between independent variables and strength of that correlation. $\\mathbf{VIF = \\frac {1}{1-R^2}}$, If VIF >1 & VIF <5 moderate correlation, VIF < 5 critical level of multicollinearity.\n6. Homoscedasticity: The data are homoscedastic meaning the residuals are equal across the regression line. We can look at residual Vs fitted value scatter plot. If heteroscedastic plot would exhibit a funnel shape pattern.","metadata":{}},{"cell_type":"code","source":"# Check for Linearity\nf = plt.figure(figsize=(14,5))\nax = f.add_subplot(121)\nsns.scatterplot(y_test,y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.distplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:51.255134Z","iopub.execute_input":"2022-05-24T13:54:51.255379Z","iopub.status.idle":"2022-05-24T13:54:51.919363Z","shell.execute_reply.started":"2022-05-24T13:54:51.255334Z","shell.execute_reply":"2022-05-24T13:54:51.918210Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(14,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:51.921043Z","iopub.execute_input":"2022-05-24T13:54:51.921677Z","iopub.status.idle":"2022-05-24T13:54:52.450234Z","shell.execute_reply.started":"2022-05-24T13:54:51.921585Z","shell.execute_reply":"2022-05-24T13:54:52.449091Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF","metadata":{"execution":{"iopub.status.busy":"2022-05-24T13:54:52.451947Z","iopub.execute_input":"2022-05-24T13:54:52.452582Z","iopub.status.idle":"2022-05-24T13:54:52.461586Z","shell.execute_reply.started":"2022-05-24T13:54:52.452499Z","shell.execute_reply":"2022-05-24T13:54:52.460211Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"The model assumption linear regression as follows\n1. In our model  the actual vs predicted plot is curve so linear assumption fails\n2. The residual mean is zero and residual error plot right skewed\n3. Q-Q plot shows as value log value greater than 1.5 trends to increase\n4. The plot is exhibit heteroscedastic, error will insease after certian point.\n5. Variance inflation factor value is less than 5, so no multicollearity.","metadata":{}}]}